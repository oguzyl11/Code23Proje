import streamlit as st
import os

# ==========================================
# ğŸ”‘ ANAHTARI BURAYA YAPIÅTIR
# ==========================================
MY_API_KEY = "AIzaSyDKqVM_cGh3ceLzd4V-t58QhgodRnZZ4Yc"
os.environ["GOOGLE_API_KEY"] = MY_API_KEY

# --- IMPORTLAR ---
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate

# --- SAYFA AYARLARI ---
st.set_page_config(page_title="Mevzuat AsistanÄ± v2", layout="wide")
st.title("ğŸ§  2. Hafta: GeliÅŸmiÅŸ RAG (MMR & Prompt AyarÄ±)")

# --- SIDEBAR ---
with st.sidebar:
    st.header("âš™ï¸ GeliÅŸmiÅŸ Ayarlar")
    uploaded_file = st.file_uploader("PDF YÃ¼kle", type="pdf")

    st.divider()

    # 2. HAFTA YENÄ°LÄ°ÄÄ°: Arama Tipi SeÃ§imi
    search_type = st.radio(
        "Arama YÃ¶ntemi (Retriever)",
        ["Similarity (Benzerlik)", "MMR (Ã‡eÅŸitlilik)"]
    )

    chunk_size = st.slider("Chunk Size", 500, 2000, 1000)
    k_value = st.slider("k DeÄŸeri (ParÃ§a SayÄ±sÄ±)", 1, 10, 3)

    if st.button("Sohbeti Temizle"):
        st.session_state.clear()
        st.rerun()


# --- FONKSÄ°YONLAR ---
def process_pdf(file, chunk_s):
    with open("temp.pdf", "wb") as f:
        f.write(file.getbuffer())

    loader = PyPDFLoader("temp.pdf")
    pages = loader.load()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_s,
        chunk_overlap=150  # Overlap'i biraz artÄ±rdÄ±k, baÄŸlam kopmasÄ±n diye
    )
    docs = splitter.split_documents(pages)
    return docs


def get_vectorstore(docs):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    db = Chroma.from_documents(docs, embeddings)
    return db


# --- UYGULAMA AKIÅI ---
if uploaded_file:

    # PDF Ä°ÅŸleme
    if "db" not in st.session_state:
        with st.spinner("PDF Analiz Ediliyor..."):
            try:
                docs = process_pdf(uploaded_file, chunk_size)
                st.session_state.db = get_vectorstore(docs)
                st.success(f"Ä°ÅŸlem Tamam! {len(docs)} parÃ§a oluÅŸturuldu.")
            except Exception as e:
                st.error(f"Hata: {e}")

    # Soru Sorma
    question = st.text_input("Sorunuz:", placeholder="Ã–rn: Disiplin cezasÄ±na itiraz sÃ¼resi nedir?")

    if question and "db" in st.session_state:
        # LLM (En gÃ¼Ã§lÃ¼ model)
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.1)

        # 2. HAFTA YENÄ°LÄ°ÄÄ°: GeliÅŸmiÅŸ Prompt
        prompt_template = """
        Sen uzman bir Ã¼niversite mevzuat asistanÄ±sÄ±n.
        AÅŸaÄŸÄ±daki baÄŸlamÄ± kullanarak soruyu cevapla.

        KURALLAR:
        1. Ã–nce cevabÄ±n 1 cÃ¼mlelik net bir Ã¶zetini yaz.
        2. ArdÄ±ndan detaylarÄ± madde madde sÄ±rala.
        3. EÄŸer cevap metinde yoksa "Bilgim yok" de, asla uydurma.

        BaÄŸlam: {context}
        Soru: {question}

        Cevap:
        """
        PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

        # 2. HAFTA YENÄ°LÄ°ÄÄ°: Retriever MantÄ±ÄŸÄ±
        if search_type == "MMR (Ã‡eÅŸitlilik)":
            # MMR: Benzer ama birbirinden farklÄ± parÃ§alarÄ± getirir
            retriever = st.session_state.db.as_retriever(
                search_type="mmr",
                search_kwargs={"k": k_value, "fetch_k": 20, "lambda_mult": 0.5}
            )
        else:
            # Similarity: Sadece en Ã§ok benzeyenleri getirir
            retriever = st.session_state.db.as_retriever(
                search_kwargs={"k": k_value}
            )

        qa_chain = RetrievalQA.from_chain_type(
            llm,
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )

        with st.spinner("Analiz ediliyor..."):
            res = qa_chain.invoke({"query": question})

            st.markdown("### ğŸ¤– Cevap:")
            st.write(res["result"])

            st.divider()
            st.caption(f"KullanÄ±lan YÃ¶ntem: **{search_type}** | ParÃ§a SayÄ±sÄ±: **{k_value}**")
            for i, doc in enumerate(res["source_documents"]):
                with st.expander(f"Kaynak {i + 1} (Sayfa {doc.metadata.get('page', 0) + 1})"):
                    st.write(doc.page_content)

elif not uploaded_file:
    st.info("BaÅŸlamak iÃ§in PDF yÃ¼kleyin.")